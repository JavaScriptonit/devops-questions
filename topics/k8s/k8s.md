## Вопросы:
1. Используются ли БД в k8s для имеющихся сервисов? Коннектятся ли БД к каким-то сервисам внутри k8s?
2. Как вносите изменения в БД когда релизите новую версию приложения и для неё нужно изменить табличку?
3. Как обходить блокировки при миграции БД в k8s при 2-3 репликах сервиса?
4. Использовал ли Jobs и Init контейнеры? Для чего? Привести пример
5. Jobs и Init контейнеры в k8s? Как работает сам механизм? Привести пример
6. Может ли Init контейнер настраивать окружение?
7. Для запуска 3х реплик приложения - сколько нужно запустить Init контейнеров? В чарте по описанию - 3 реплики
8. Для чего в промышленном использовании куба может подниматсья Job для приложений?
9. Что будет с configMap в самом кубере?
10. Как в таком случае k8s понимает что нужно удалить configMap?
11. Каким образом он отделяет внутри кубера то что было задеплоино с этим приложением от всего другого?
12. Как вы понимали что релиз удачный? Выводим что-то на прод. Как понять что всё прошло хорошо? 
13. Может быть такая ситуация что "Статус - Deployed", но при этом поды не работают или работают неправильно. Что делать чтобы этого избежать?
14. Ставим 3 реплики приложения. Заходим, а их 5 в кубере. Как будешь понимать какая реплика к какой версии относится? 
15. Ставим 3 реплики приложения. Заходим, а их 5 в кубере. Как такое может произойти при деплое?
16. Разворачивал ли в k8s PostgreSQL, Redis?
17. Что еще делал в k8s? Приведи примеры.
18. Расскажи подробно работу деплоя в k8s кластер при помощи Арго и гитлаба
19. Где запускаются джобы lint/prettier?
20. Может ли на сервере выполняться несколько джобов сразу?
21. Перечислить список типов объектов в Kubernetes.
22. Назвать разницу между Deployment и StatefulSet.
23. Рассказать что такое POD в k8s.
24. Зачем нужен k8s? (k8s)
25. Чем лучше оркестрация k8s в сравнении с systemd units и ansible, заменит ли оркестрацию? (k8s)
26. Удавалось ли самостоятельно разворачивать k8s на bare metal? Если да - то через что? (k8s)
27. Из каких служб состоит k8s? (k8s)
28. Может ли быть в pod больше 1го контейнера? И когда? (k8s)
29. В каких ситуациях может DS пригодиться? Что такое DaemonSet? (k8s)
30. Создан pod. Статус - "Image pull back off" (образ не смог стянуть). Это приватный реестр. Куда смотреть? Как исправить? (k8s)
31. Создали ingress, который смотрит на сервис, а сервис смотрит на поды. Делая запрос на ingress - получаем ошибку. Запрос до pod не доходит (запроса нет в логах пода). Как исправить? Куда смотреть? (k8s)
32. Какие отличия DaemonSet от Deployment и StatefulSet? (k8s)
33. Как происходит создание ролей в кластере? Service Account, Role Binding, Role. Чем они отличаются? (k8s)
34. Какие отличия между docker/dockerd/containerd? (k8s)
35. 

## 1. Используются ли БД в k8s для имеющихся сервисов? Коннектятся ли БД к каким-то сервисам внутри k8s?

1) В Kubernetes (k8s) часто используются базы данных для хранения данных, которые обрабатываются сервисами. БД могут быть запущены внутри кластера Kubernetes для обеспечения отказоустойчивости, масштабируемости и управления данными.

2) Да, базы данных могут подключаться к сервисам внутри кластера Kubernetes. Например, приложения внутри кластера могут использовать DNS имена для обращения к базе данных, которая также запущена в кластере.

Пример поднятия базы данных в Kubernetes с использованием волюма в одном поде и бэкэнд сервиса в другом поде в рамках одного Namespace:

### Шаги для развертывания:

1. **Создание манифестов для базы данных и бэкэнд сервиса**:

   - Создайте манифест для запуска базы данных (например, PostgreSQL) с использованием PersistentVolume и PersistentVolumeClaim для хранения данных.
   - Создайте манифест для запуска бэкэнд-сервиса, который будет подключаться к базе данных.

2. **Применение манифестов**:

   ```bash
   $ kubectl apply -f database-pod.yaml
   $ kubectl apply -f backend-service-pod.yaml
   ```

3. **Обеспечение связи между подами**:
   - В манифесте бэкэнд-сервиса укажите имя базы данных как хост для подключения.
   - Можно использовать DNS имена для обращения к другим сервисам внутри кластера Kubernetes.

Пример манифестов для базы данных и бэкэнд сервиса можно найти ниже:

### Пример манифеста для базы данных (database-pod.yaml):
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: database-pod
spec:
  containers:
  - name: database
    image: postgres
    volumeMounts:
    - mountPath: /var/lib/postgresql/data
      name: database-storage
  volumes:
  - name: database-storage
    persistentVolumeClaim:
      claimName: database-pvc
```

### Пример манифеста для бэкэнд-сервиса (backend-service-pod.yaml):
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: backend-service-pod
spec:
  containers:
  - name: backend-service
    image: backend-service-image
    env:
    - name: DATABASE_HOST
      value: database-pod
```

После применения этих манифестов, база данных и бэкэнд-сервис будут запущены в кластере Kubernetes и смогут общаться друг с другом.

## 2. Как вносите изменения в БД когда релизите новую версию приложения и для неё нужно изменить табличку? (k8s)

Для внесения изменений в базу данных при релизе новой версии приложения в Kubernetes и изменения структуры таблицы, можно использовать подходы, такие как использование миграций баз данных или управляемых решений для управления схемой базы данных.

### Миграции баз данных:
1. **Создание миграций**: Создайте скрипты миграций баз данных, которые будут выполнять необходимые изменения в структуре таблицы (например, добавление новых столбцов, изменение типов данных и т. д.).
2. **Применение миграций**: Включите выполнение этих скриптов миграций в процесс деплоя новой версии вашего бэкэнд-приложения. Например, вы можете использовать инструменты для миграции баз данных, такие как Flyway или Liquibase, которые могут автоматически применять миграции при запуске новой версии приложения.

### Пример использования миграций баз данных:
1. Создайте новый скрипт миграции, который содержит SQL-запросы для изменения структуры вашей таблицы.
2. Обновите манифест вашего бэкэнд-сервиса в Kubernetes, чтобы включить применение этого скрипта миграции при деплое новой версии приложения.
3. После успешного деплоя новой версии приложения, скрипт миграции будет выполнен, и изменения в структуре таблицы будут применены.

### Важно:
- При использовании миграций баз данных, убедитесь в тщательном тестировании скриптов миграций перед их применением в производственной среде.
- Резервируйте резервные копии базы данных перед внесением изменений, чтобы в случае неудачного применения миграции можно было быстро восстановить данные.

После внесения изменений в базу данных с помощью миграций, ваше приложение с новой версией сможет корректно работать с обновленной структурой таблицы.

### Пример миграции:

В качестве примера применения скриптов миграций для изменения структуры таблицы в базе данных MySQL при деплое новой версии приложения в Kubernetes, рассмотрим следующий сценарий:

1. **Создание скрипта миграции**:
   - Создайте новый SQL-скрипт, например, `001_alter_table.sql`, который содержит запросы для изменения структуры таблицы. Например, добавим новый столбец `new_column` в таблицу `example_table`:
     ```sql
     ALTER TABLE example_table
     ADD COLUMN new_column VARCHAR(50);
     ```

2. **Применение миграции при деплое новой версии приложения**:
   - Включите выполнение этого скрипта миграции в процесс деплоя новой версии вашего бэкэнд-приложения в Kubernetes. Для этого в манифесте вашего приложения (например, Deployment) добавьте инициализацию контейнера для выполнения скрипта миграции перед запуском приложения.
   - Пример манифеста Deployment с инициализацией контейнера для выполнения скрипта миграции:
     ```yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: backend-app
     spec:
       replicas: 3
       template:
         spec:
           containers:
           - name: backend
             image: your-backend-image:latest
           initContainers:
           - name: migrate
             image: mysql:latest
             command: ["mysql", "-h", "your-mysql-host", "-u", "your-username", "-pYourPassword", "your-database", "-e", "source /path/to/001_alter_table.sql"]
     ```

3. **Применение изменений**:
   - После успешного деплоя новой версии приложения в Kubernetes, скрипт миграции `001_alter_table.sql` будет выполнен, и изменения в структуре таблицы будут применены к базе данных.

### Примечания:
- Убедитесь, что у вас есть доступ к базе данных MySQL и правильно указаны параметры подключения в команде MySQL в манифесте.
- Перед применением миграции в производственной среде, убедитесь в тщательном тестировании скрипта миграции на тестовой базе данных.

Это пример использования скриптов миграций для изменения структуры таблицы при деплое новой версии приложения в Kubernetes. Помните о важности тестирования и резервирования данных перед внесением изменений в базу данных.

## 3. Как обходить блокировки при миграции БД в k8s при 2-3 репликах сервиса? (k8s)
### Миграции БД:
1. 1 из решений - плохое - это подключиться к базе вручную и ручками добавить изменения в базу.
2. 2 из решений - это накатывать из запущенного приложения изменения на базу
Но когда это запускается в кубере то тут зависит от кол-ва экземпляров сервиса.
Если подняты 2-3 реплики и они одновременно пойдут прогонять миграции по БД то могут быть блокировки

При работе с миграциями баз данных в Kubernetes с несколькими репликами сервиса можно столкнуться с проблемой блокировок, если все реплики одновременно пытаются применить миграции к базе данных. Это может привести к конфликтам и нежелательным результатам.

Для обхода блокировок при миграции баз данных в Kubernetes при наличии 2-3 реплик сервиса можно использовать следующие подходы:

1) Одноразовые миграции: Вы можете настроить механизм, который позволит запускать миграции только из одной реплики сервиса. Например, вы можете использовать Job в Kubernetes для запуска миграций и убедиться, что Job будет выполнен только один раз, независимо от количества реплик сервиса.

2) Блокировка базы данных: Вы можете использовать механизм блокировки базы данных, чтобы предотвратить одновременное выполнение миграций из нескольких реплик сервиса. Например, вы можете использовать специальный инструмент для управления блокировками базы данных или самостоятельно реализовать механизм блокировки в коде миграций.

3) Расписание миграций: Вы можете настроить расписание для миграций баз данных, чтобы они запускались последовательно и не конфликтовали между собой. Например, вы можете использовать CronJob в Kubernetes для запуска миграций по расписанию.

Выбор конкретного подхода зависит от особенностей вашей системы и требований к процессу миграции баз данных в Kubernetes. Важно тщательно протестировать выбранный подход перед его внедрением в производственную среду, чтобы избежать возможных проблем и конфликтов.

## 4. Использовал ли Jobs и Init контейнеры? Для чего? Привести пример (k8s)

Jobs и Init контейнеры в Kubernetes используются для выполнения определенных задач в рамках пода или приложения. Вот примеры использования Jobs и Init контейнеров в Kubernetes:

1. **Использование Job для выполнения одноразовой задачи:**

Создадим Job, который будет выполняться один раз и завершаться после выполнения задачи.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-job
spec:
  template:
    spec:
      containers:
      - name: my-container
        image: busybox
        command: ["echo", "Hello from the Job"]
      restartPolicy: Never
  backoffLimit: 4
```

Запустим Job с помощью команды:
```bash
kubectl apply -f job.yaml
```

2. **Использование Init контейнера для предварительной настройки:**

Создадим Pod с Init контейнером, который будет выполняться перед основным контейнером приложения.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: main-container
    image: nginx
  initContainers:
  - name: init-container
    image: busybox
    command: ['sh', '-c', 'echo "Init container is running"']
```

Запустим Pod с Init контейнером с помощью команды:
```bash
kubectl apply -f pod.yaml
```

3. **Пример использования Init контейнера для инициализации базы данных:**

Создадим Pod с Init контейнером, который будет инициализировать базу данных перед запуском основного приложения.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: db-pod
spec:
  containers:
  - name: db-container
    image: mysql
    env:
      - name: MYSQL_ROOT_PASSWORD
        value: password
  initContainers:
  - name: init-db
    image: mysql
    command: ['sh', '-c', 'mysql -u root -ppassword -e "CREATE DATABASE mydb;"']
```

Запустим Pod с Init контейнером для инициализации базы данных:
```bash
kubectl apply -f db-pod.yaml
```

Это лишь примеры использования Jobs и Init контейнеров в Kubernetes. Важно помнить, что Jobs используются для одноразовых задач, а Init контейнеры для предварительной настройки перед запуском основного контейнера приложения.

## 5. Jobs и Init контейнеры в k8s? Как работает сам механизм? Привести пример (k8s)

Jobs и Init контейнеры - это два различных концепта в Kubernetes, которые используются для выполнения определенных задач в рамках пода или приложения. Давайте рассмотрим их работу более подробно:

1. **Init контейнеры:**

Init контейнеры представляют собой дополнительные контейнеры, которые запускаются перед основным контейнером приложения в рамках одного и того же пода. Они используются для предварительной настройки среды перед запуском основного контейнера. Когда все Init контейнеры завершают свою работу успешно, основной контейнер приложения запускается.

Механизм работы Init контейнеров в Kubernetes:
- Когда Pod создается, Kubernetes запускает все Init контейнеры в порядке их определения в спецификации Pod.
- Каждый Init контейнер выполняет свою задачу и завершается.
- После того как все Init контейнеры успешно завершились, основной контейнер приложения стартует.

2. **Jobs:**

Jobs в Kubernetes используются для выполнения одноразовых задач, которые должны быть выполнены успешно. Job создает один или несколько подов (Pod) и гарантирует, что задача будет выполнена успешно. После завершения задачи, Pod удаляется.

Механизм работы Jobs в Kubernetes:
- Job создает один или несколько подов (Pod), которые выполняют задачу.
- Kubernetes отслеживает состояние выполнения Job и подов.
- После завершения задачи, Pod удаляется, но информация о выполнении задачи сохраняется в истории Job.

Таким образом, Init контейнеры используются для предварительной настройки среды перед запуском основного контейнера приложения, а Jobs используются для выполнения одноразовых задач. Оба эти концепта помогают управлять различными аспектами запуска и выполнения приложений в Kubernetes.

## 6. Может ли Init контейнер настраивать окружение? (k8s)

1) Да, инит контейнер может настраивать окружение внутри пода. Например, он может выполнять следующие задачи:
   - Установка зависимостей и пакетов перед запуском основного приложения.
   - Проверка доступности внешних сервисов или ресурсов, необходимых для работы приложения.
   - Инициализация базы данных или других хранилищ данных перед запуском приложения.

2) Когда говорим о "предварительной настройке среды", мы имеем в виду подготовку условий для запуска основного контейнера приложения. Это может включать в себя настройку файловой системы, установку необходимых инструментов, проверку доступности ресурсов и многое другое.

Пример использования инит контейнера для настройки окружения:
Предположим, у вас есть приложение, которое требует подключения к базе данных перед запуском. Вы можете использовать инит контейнер для инициализации базы данных перед запуском основного контейнера приложения.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app-container
    image: myapp
  initContainers:
  - name: init-db
    image: mysql
    command: ['sh', '-c', 'mysql -u root -ppassword -e "CREATE DATABASE mydb;"']
```

В этом примере инит контейнер запускает команду для создания базы данных MySQL перед запуском основного контейнера приложения. Таким образом, инит контейнер помогает предварительно настроить среду (в данном случае - базу данных) перед запуском приложения.

## 7. Для запуска 3х реплик приложения - сколько нужно запустить Init контейнеров? В чарте по описанию - 3 реплики  (k8s)

Для запуска 3 реплик приложения в Kubernetes, вам необходимо запустить по одному инит контейнеру для каждой реплики. Каждый инит контейнер будет выполняться в каждой реплике перед запуском основного контейнера приложения.

Таким образом, если у вас есть приложение с 3 репликами, вам нужно будет запустить 3 инит контейнера - по одному для каждой реплики. Каждый из этих инит контейнеров будет выполнять задачи предварительной настройки среды перед запуском соответствующей реплики основного контейнера приложения.

## 8. Для чего в промышленном использовании куба может подниматсья Job для приложений? (k8s)

Пример создания Job в Kubernetes:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: example-job
spec:
  template:
    spec:
      containers:
      - name: example-container
        image: busybox
        command: ['echo', 'Hello, Kubernetes!']
      restartPolicy: Never
  backoffLimit: 5
```

В промышленном использовании Kubernetes Job может подниматься для выполнения одноразовых задач, таких как:
- Запуск скриптов обслуживания базы данных или других ресурсов.
- Выполнение регулярных задач по расписанию, таких как резервное копирование данных.
- Обновление индексов или кэшей.
- Выполнение операций по обработке данных или аналитике.

В вашем примере, где у вас есть 3 реплики фронт/бэк приложения в Kubernetes, Job может подниматься для выполнения регулярных задач, таких как очистка временных файлов, обновление кэшей, резервное копирование баз данных и т. д.

Пример конфигурации перезапуска Job и его расписания:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: data-backup-job
spec:
  template:
    spec:
      containers:
      - name: backup-container
        image: backup-image
        command: ['sh', '-c', 'backup-script.sh']
      restartPolicy: OnFailure
  backoffLimit: 3
  schedule: "0 0 * * *"
```

В этом примере Job будет выполняться каждый день в полночь для выполнения задачи резервного копирования данных. Если Job завершится с ошибкой, он будет перезапущен не более 3 раз. Таким образом, Job будет выполняться каждый день для обеспечения регулярного резервного копирования данных вашего приложения.

## 9. Что будет с configMap в самом кубере? (helm) (k8s)
Есть очень простенький чарт в helm версии 1.0.
Мы там создаём конфиг мапу, создаём configMap.yaml, описываем его, деплоим в кубер.
Далее меняем чарт не в кубере, а у себя в файликах и меняем версию с 1.0 на 1.1 и удаляем эту конфиг мапу, деплоимся еще раз.
Вопрос:
Что будет с configMap в самом кубере?
-----------------------

Если вы изменили ваш Helm чарт локально, удалили конфигмапу и обновили версию чарта с 1.0 на 1.1, затем развернули обновленный чарт в Kubernetes, то следующее произойдет с конфигмапой в самом Kubernetes:

1. При развертывании обновленного чарта с версией 1.1, Helm не создаст конфигмапу, так как она была удалена из вашего чарта.
2. Существующая конфигмапа, которая была создана при развертывании чарта версии 1.0, останется в Kubernetes без изменений.
3. Kubernetes не удалит конфигмапу автоматически, так как она была создана Helm'ом и не управляется им после создания.

Таким образом, конфигмапа, которая была создана при развертывании чарта версии 1.0, останется в Kubernetes после обновления чарта на версию 1.1 и удаления конфигмапы из чарта.

## 10. Как в таком случае k8s понимает что нужно удалить configMap? (helm) (k8s)

При обновлении Helm чарта и удалении конфигмапы из его конфигурации, Kubernetes удалит существующую конфигмапу. 

Kubernetes понимает, что конфигмапа должна быть удалена из-за управления ресурсами через контроллеры ресурсов. При обновлении ресурса Deployment, StatefulSet или других контроллеров, Kubernetes сравнивает текущее состояние ресурсов с новой конфигурацией и применяет необходимые изменения.

Если конфигмапа была создана Helm'ом при развертывании чарта версии 1.0, а затем удалена из чарта версии 1.1, при следующем деплое Helm чарта версии 1.1 Kubernetes обнаружит, что конфигмапа больше не упоминается в конфигурации чарта и удалит ее из кластера.

Таким образом, при обновлении чарта и удалении конфигмапы из его конфигурации, Kubernetes удалит существующую конфигмапу, так как она больше не описывается в конфигурации чарта.

## 11. Каким образом он отделяет внутри кубера то что было задеплоино с этим приложением от всего другого? (helm) (k8s)
У нас есть 2 множества: 1ое - это множество объектов в кубере. В нём может быть больше чем наше 1 приложение и есть множество объектов внутри Чарта, который мы деплоим.
Хелм видит 2ое множество, которое внутри чарта. Но ему нужно как-то сравнить 1ое множество внутри кубера и 2ое множество внутри чарта.
Кубер же не может удалить всё чего нет в нашем чарте (например, что-то поставлено руками, а что-то другими чартами).
Каким образом он отделяет внутри кубера то что было задеплоино с этим приложением от всего другого?
-----------------------

Kubernetes использует концепцию меток (labels) и селекторов (selectors) для управления и организации ресурсов в кластере. Это помогает ему отслеживать и управлять объектами, связанными с конкретным приложением или чартом Helm.

Когда вы развертываете приложение с помощью Helm чарта, вы можете добавить метки к создаваемым ресурсам (например, Deployment, Service, ConfigMap и т. д.). Эти метки могут указывать на принадлежность ресурсов к конкретному приложению или чарту.

При обновлении или удалении ресурсов через Helm, он использует селекторы, чтобы выбрать именно те ресурсы, которые ему нужно обновить или удалить. Это позволяет Helm правильно управлять только теми ресурсами, которые были созданы или изменены через него.

Если какие-то ресурсы были созданы в кластере вне контекста Helm или текущего приложения, Kubernetes не будет автоматически удалять их при обновлении Helm чарта. Однако, правильное использование меток и селекторов помогает отделить ресурсы, созданные в рамках вашего приложения, от других ресурсов в кластере и обеспечить их правильное управление.

### Пример, демонстрирующий как Helm версии 3 добавляет метки к ресурсам, создаваемым в рамках чарта:

В Helm версии 3 информация о ресурсах, созданных в рамках управления чартом, хранится в секрете (Secret) внутри кластера Kubernetes. Этот секрет содержит метаданные о ресурсах, такие как их идентификаторы, типы и метки.

```yaml
# values.yaml
app:
  name: my-app
  replicaCount: 3
```

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-{{ .Values.app.name }}-deployment
  labels:
    app: {{ .Values.app.name }}
    chart: {{ .Chart.Name }}-{{ .Chart.Version }}
spec:
  replicas: {{ .Values.app.replicaCount }}
  selector:
    matchLabels:
      app: {{ .Values.app.name }}
    template:
      metadata:
        labels:
          app: {{ .Values.app.name }}
      spec:
        containers:
          - name: my-app-container
            image: nginx:latest
```

В этом примере, когда Helm разворачивает чарт, он добавляет метки `app` и `chart` к создаваемому Deployment. Значение метки `app` соответствует имени приложения, указанного в `values.yaml`, а значение метки `chart` содержит имя и версию чарта.

Эти метки помогают Helm отслеживать и управлять ресурсами, созданными в рамках чарта, а также обеспечивают целостность и безопасность управления ресурсами в кластере Kubernetes.

## 12. Как вы понимали что релиз удачный? Выводим что-то на прод. Как понять что всё прошло хорошо? (k8s)

Для команды DevOps важно иметь надежные и эффективные методы анализа успешности релиза при развертывании на продукционную среду в Kubernetes. Вот несколько best practices для анализа успешности релиза:

1. **Health Checks (Проверка состояния)**:
   - Используйте readiness и liveness probes в Kubernetes для проверки состояния приложения после развертывания. Readiness probe позволяет Kubernetes понять, когда приложение готово обслуживать запросы, а liveness probe проверяет, что приложение работает корректно.
  
2. **Мониторинг**:
   - Настройте мониторинг с помощью инструментов, таких как Prometheus и Grafana, для отслеживания производительности, доступности и состояния вашего приложения после развертывания.
  
3. **Логирование**:
   - Собирайте и анализируйте логи приложения и инфраструктуры с помощью инструментов, например Elasticsearch и Kibana, для быстрого выявления проблем и отладки при необходимости.
  
4. **Тестирование**:
   - Автоматизируйте тестирование вашего приложения перед развертыванием на продукционную среду. Включите юнит-тесты, интеграционные тесты и end-to-end тесты, чтобы убедиться в работоспособности приложения.
  
5. **Роллбэк**:
   - Подготовьте план роллбэка в случае неудачного релиза. Используйте механизмы Kubernetes, такие как Deployment Rollback или Helm Rollback, чтобы быстро вернуться к предыдущей стабильной версии приложения.
  
6. **Мониторинг событий**:
   - Следите за событиями и уведомлениями Kubernetes для оперативного реагирования на любые проблемы или нештатные ситуации.
  
7. **Post-Deployment Checks (Проверки после развертывания)**:
   - Проводите проверки после развертывания, чтобы убедиться, что все компоненты приложения работают корректно и соответствуют ожиданиям.

Команда DevOps должна активно использовать эти best practices для анализа успешности релиза на продукционной среде в Kubernetes. Постоянное улучшение и оптимизация процесса развертывания помогут обеспечить стабильную и надежную работу приложения в продакшене.

## 13. Может быть такая ситуация что "Статус - Deployed", но при этом поды не работают или работают неправильно. Что делать чтобы этого избежать? (helm) (k8s)

Чтобы избежать подобных ситуаций и обеспечить надежность развертывания, рекомендуется следующее:

1. **Добавление автоматизированных тестов**: Включите автоматизированные тесты в пайплайн CI/CD, которые будут проверять работоспособность приложения после развертывания. Это могут быть тесты доступности, функциональные тесты, нагрузочное тестирование и т.д.

2. **Использование Readiness и Liveness probes**: Убедитесь, что контейнеры настроены с помощью readiness и liveness probes, чтобы Kubernetes мог мониторить состояние приложения и перезапускать поды в случае проблем.

3. **Мониторинг и логирование**: Настройте мониторинг и логирование для вашего кластера Kubernetes, чтобы оперативно обнаруживать и реагировать на проблемы после развертывания.

4. **Ручная проверка**: Важно также проводить ручные проверки после развертывания, чтобы убедиться, что приложение работает корректно и доступно для пользователей.

Путем комбинации автоматизированных тестов, мониторинга, настройки probes и ручной проверки вы сможете обеспечить более надежное и стабильное развертывание вашего приложения на кластер Kubernetes.

## 14. Ставим 3 реплики приложения. Заходим, а их 5 в кубере. Как будешь понимать какая реплика к какой версии относится? (helm) (k8s)

Для определения, к какой версии относится каждая реплика приложения в Kubernetes, можно использовать метаданные, такие как метки (labels) и аннотации (annotations). В случае использования Helm для управления релизами, Helm автоматически добавляет метаданные к ресурсам Kubernetes, что облегчает определение связи между репликами и версиями приложения.

Вот примеры команд и шагов для проверки связи между репликами и версиями приложения:

1. **Использование kubectl**:
   - Шаг 1: Запустите команду `kubectl get pods` для получения списка всех подов приложения.
   - Шаг 2: Найдите метаданные (labels, annotations) каждого пода, например, с помощью команды `kubectl describe pod <pod_name>`.
   - Шаг 3: Определите, какие метки или аннотации указывают на версию приложения или релиз Helm.

2. **Использование Helm**:
   - Шаг 1: Запустите команду `helm list` для получения списка всех установленных релизов Helm.
   - Шаг 2: Найдите имя релиза, связанного с вашим приложением.
   - Шаг 3: Запустите команду `helm get values <release_name>` для просмотра значений, используемых при установке релиза. Обычно там указывается версия приложения.

Что касается Argo CD, вы также можете использовать его для просмотра информации о развернутых релизах и связанных с ними ресурсах Kubernetes. В Argo CD вы можете просмотреть манифесты, связанные с каждым релизом, и найти информацию о версии приложения, метках и других метаданных.

Таким образом, для определения связи между репликами и версиями приложения в Kubernetes с Helm или Argo CD, вы можете использовать метаданные ресурсов Kubernetes и информацию о релизах, предоставляемую инструментами управления развертыванием.

## 15. Ставим 3 реплики приложения. Заходим, а их 5 в кубере. Как такое может произойти при деплое? (helm) (k8s)
Горизонтальное масштабирование выключено, но мы ставим что всегда хотим видеть 3 реплики этого приложения. Делаем деплой и есть ситуации когда у нас поднимутся 2-3 пода сверху и старые еще не будут убиты.
-----------------------

1) Если при деплое с помощью Helm или Kubernetes у вас появляется больше реплик, чем ожидалось, это может быть вызвано несколькими причинами:

- **Ошибки в манифестах**: Возможно, в манифестах Helm или Kubernetes указано создание большего количества реплик, чем вы ожидали.
- **Проблемы с автомасштабированием**: Если у вас настроено автомасштабирование (Horizontal Pod Autoscaler), это может привести к автоматическому созданию дополнительных реплик в зависимости от нагрузки.
- **Проблемы с контроллерами реплик**: Неправильные настройки контроллеров реплик могут привести к созданию дополнительных реплик.

2) Если горизонтальное масштабирование выключено, но приложение запускает больше реплик, чем ожидалось, это может быть связано с задержкой в остановке старых подов при обновлении. Это может произойти из-за различных факторов, таких как время завершения процесса завершения пода, наличие долгих операций завершения или неправильной конфигурации контроллеров реплик.

Для решения этой проблемы и обеспечения того, что всегда видно только 3 реплики приложения, можно принять следующие меры:

- **Установка максимального числа реплик**: В манифестах Helm или Kubernetes можно указать максимальное количество реплик для контроллера, чтобы избежать появления дополнительных реплик.
- **Мониторинг и логирование**: Отслеживайте количество запущенных реплик и реагируйте на любые аномалии.
- **Использование Readiness и Liveness probes**: Настройте проверки готовности и жизнеспособности для ваших подов, чтобы убедиться, что новые реплики не запускаются до завершения процесса завершения старых.

Принимая эти меры, вы сможете обеспечить стабильное и предсказуемое количество реплик вашего приложения в Kubernetes даже без горизонтального масштабирования.

## 16. Разворачивал ли в k8s PostgreSQL, Redis? (k8s)
https://habr.com/ru/companies/domclick/articles/649167/ - Разворачиваем PostgreSQL, Redis и RabbitMQ в Kubernetes-кластере

### Руководство по развертыванию базы данных в Kubernetes

1) **Создание Persistent Volume (PV) и Persistent Volume Claim (PVC):**

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

2) **Установка Helm-чарта целевого приложения:**

```bash
$ helm repo add bitnami https://charts.bitnami.com/bitnami
$ helm install dev-pg bitnami/postgresql --set primary.persistence.existingClaim=pg-pvc,auth.postgresPassword=pgpass
```

3) **Проверка работы:**

```bash
$ kubectl get pvc
$ kubectl get pod,statefulset
```

4) **Перед началом работ нужно минимально настроить кластер Kubernetes.**
   - Версия Kubernetes 1.20+
   - Одна master-нода и одна worker-нода
   - Настроенный Ingress-controller
   - Установлен Helm

5) **Создание ресурса StorageClass и Применение манифеста:**

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

6) **Создание ресурса Persistent Volume:**
В matchExpressions указываем название ноды, на которой будет монтироваться диск. Посмотреть имя доступных узлов можно с помощью команды: `kubectl get nodes`

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-for-pg
  labels:
    type: local
spec:
  capacity:
    storage: 4Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /devkube/postgresql
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - 457344.cloud4box.ru
```

7) **Для удобства монтируем диск на мастер-ноде:**

```bash
$ mkdir -p /devkube/postgresql
```

8) **Применение манифеста Persistent Volume:**

```bash
$ kubectl apply -f pv.yaml
```

9) **Проверка состояния:**

```bash
$ kubectl get pv
```

10) **Применение манифеста Persistent Volume Claim:**

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pg-pvc
spec:
  storageClassName: "local-storage"
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi
```

11) **Проверка состояния ресурса PVC:**
Ресурс PVC в ожидании привязки. 

```bash
$ kubectl get pvc
```

12) **Подтягиваем к себе репозиторий Bitnami и Устанавливаем Helm-чарт с Postgres:**

```bash
$ helm repo add bitnami https://charts.bitnami.com/bitnami

$ helm install dev-pg bitnami/postgresql --set primary.persistence.existingClaim=pg-pvc,auth.postgresPassword=pgpass
```

13) **Проверка состояния PVC:**
теперь pod с Postgres будет писать данные в директорию /devkube/postgresql.

```bash
$ kubectl get pvc
```

14) **Проверка состояния Pod и StatefulSet:**

```bash
$ kubectl get pod,statefulset
```

### База успешно развёрнута, теперь попробуем подключиться к ней и создать пользователя, таблицу и настроить доступы. После установки чарта в консоли будут показаны некоторые способы подключения к БД. Есть два способа:

15.1) **Подключение к БД:**

   - Проброс порта на локальную машину:
     ```bash
     $ kubectl port-forward --namespace default svc/dev-pg-postgresql 5432:5432
     ```
   - Подключение к БД:
     ```bash
     $ psql --host 127.0.0.1 -U postgres -d postgres -p 5432
     ```

15.2) **Создание поды с psql клиентом:**

```bash
$ kubectl run dev-pg-postgresql-client --rm --tty -i --restart='Never' --namespace default --image docker.io/bitnami/postgresql:14.2.0-debian-10-r22 --env="PGPASSWORD=$POSTGRES_PASSWORD" \
      --command -- psql --host dev-pg-postgresql -U postgres -d postgres -p 5432
```

16) **Создание роли и пароля для пользователя:**

```sql
CREATE ROLE qa_user WITH LOGIN ENCRYPTED PASSWORD 'qa-pg-pass';
```

17) **Создание базы данных с владельцем qa_user:**

```sql
CREATE DATABASE qa_db OWNER qa_user;
```

18) **Подключение к базе данных с новым пользователем:**

```bash
$ kubectl run dev-pg-postgresql-client --rm --tty -i --restart='Never' --namespace default --image docker.io/bitnami/postgresql:14.2.0-debian-10-r22 --env="PGPASSWORD=qa-pg-pass"  --command -- psql --host dev-pg-postgresql -U qa_user -d qa_db -p 5432
```

19) **База успешно развёрнута!**
    - Адрес БД для приложения: `DATABASE_URI=postgresql://qa_user:qa-pg-pass@dev-pg-postgresql:5432/qa_db`

Это руководство поможет вам успешно развернуть и настроить базу данных в Kubernetes.

## 17. Что еще делал в k8s? Приведи примеры. (k8s)

В качестве Senior DevOps Engineer работал с Kubernetes, я выполнял следующие задачи и могу похвастаться следующими достижениями:

1) **Горизонтальное масштабирование приложения:**

- **Задача:** Настройка автоматического горизонтального масштабирования для приложения в Kubernetes.
- **Шаги:**
  - Создание горизонтального масштабирования для деплоя приложения:

```bash
$ kubectl autoscale deployment <deployment-name> --cpu-percent=70 --min=3 --max=10
```

- **Конфигурационный файл для автомасштабирования:**

```yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: <hpa-name>
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: <deployment-name>
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
```

2) **Управление секретами и конфигурациями:**

- **Задача:** Безопасное хранение и использование секретов и конфигураций в Kubernetes.
- **Шаги:**
  - Создание секрета для базы данных:

```bash
$ kubectl create secret generic db-credentials --from-literal=username=db_user --from-literal=password=db_password
```

- **Использование секрета в Pod:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: mycontainer
      image: nginx
      env:
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
```

3) **Настройка мониторинга и логирования:**

- **Задача:** Настройка мониторинга и сбора логов для кластера Kubernetes.
- **Шаги:**
  - Установка Prometheus и Grafana для мониторинга:

```bash
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/grafana-prometheus.yaml
```

- **Настройка сбора логов с помощью Fluentd:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      format json
    </source>

    <match kubernetes.**>
      @type elasticsearch
      host elasticsearch
      port 9200
      logstash_format true
      logstash_prefix kubernetes
      include_tag_key true
      tag_key @log_name
      flush_interval 5s
    </match>
```

Эти примеры сложных задач в Kubernetes для Senior DevOps Engineers позволяют продемонстрировать опыт работы с расширенными функциями и возможностями платформы.

## 18. Расскажи подробно работу деплоя в k8s кластер при помощи Арго и гитлаба. (k8s)

**Деплой в Kubernetes кластер при помощи Argo и GitLab:**

**Шаги для настройки:**

1. **Установка Argo CD:**

```bash
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

2. **Создание GitLab CI/CD pipeline:**

Создайте файл `.gitlab-ci.yml` в корне вашего репозитория `portal-ui`:

```yaml
stages:
  - deploy

deploy:
  stage: deploy
  image: argoproj/argocd-cli:v2.1.2
  script:
    - argocd login <argo-cd-server-url> --username admin --password <argo-cd-initial-password> --insecure
    - argocd app sync portal-ui --sync-option Prune=true
```

3. **Создание Argo CD приложения:**

Создайте файл `argo-application.yaml`:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: portal-ui
spec:
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: default
  project: default
  source:
    repoURL: 'https://gitlab.com/your-repo/portal-ui'
    path: .
    targetRevision: HEAD
```

4. **Изменение настроек GitLab:**

Настройте переменные окружения в GitLab для хранения данных для доступа к Argo CD.

**Что делает Argo и пример деплоя в Kubernetes кластер без Argo:**

Argo - это инструмент для управления непрерывной поставкой и развертыванием приложений в Kubernetes. Он позволяет автоматизировать процессы CI/CD и управлять развертыванием приложений в кластере.

Пример деплоя в Kubernetes кластер без Argo:
- Ручное создание манифестов Kubernetes (Deployment, Service, Ingress и т. д.).
- Применение манифестов с помощью `kubectl apply -f`.

Отличия при использовании Argo:
- Автоматизация процесса деплоя приложений в Kubernetes.
- Возможность управления развертыванием приложений из Git-репозитория.
- Визуализация состояния и истории развертываний.
- Встроенные инструменты для управления версиями приложений.

Использование Argo упрощает и автоматизирует процесс развертывания приложений в Kubernetes, делая его более надежным и эффективным.

## 19. Где запускаются джобы lint/prettier? (k8s)

Для запуска джобов lint/prettier в Kubernetes с использованием GitLab Runner и выполнения проверок кода в контейнерах в Kubernetes, вам нужно настроить GitLab Runner в Kubernetes и определить задачи в `.gitlab-ci.yml` для запуска контейнеров с инструментами для проверки кода.

Вот пример шагов и команд для поднятия GitLab Runner в Kubernetes и выполнения джобов в контейнерах:

1. **Шаги:**

   - **Шаг 1: Установка GitLab Runner в Kubernetes:**
     - Установите GitLab Runner в Kubernetes с помощью Helm Chart или манифестов Kubernetes.
     - Настройте GitLab Runner для регистрации в вашем GitLab проекте.

   - **Шаг 2: Определение задач в `.gitlab-ci.yml`:**
     - В вашем `.gitlab-ci.yml` определите задачи для проверки кода, например, lint и prettier.
     - Для каждой задачи определите образ контейнера с необходимыми инструментами для проверки кода.

2. **Пример `.gitlab-ci.yml`:**

```yaml
lint:
  stage: test
  image: node:14
  script:
    - npm install eslint --save-dev
    - npm run lint

prettier:
  stage: test
  image: node:14
  script:
    - npm install prettier --save-dev
    - npm run prettier
```

3. **Пример команд для установки GitLab Runner в Kubernetes:**

```bash
# Добавление репозитория Helm Chart GitLab Runner
helm repo add gitlab https://charts.gitlab.io

# Установка GitLab Runner с помощью Helm Chart
helm install gitlab-runner gitlab/gitlab-runner -n gitlab --set gitlabUrl=<YOUR_GITLAB_URL> --set runnerRegistrationToken=<YOUR_RUNNER_TOKEN>
```

После установки GitLab Runner в Kubernetes и определения задач в `.gitlab-ci.yml`, GitLab Runner будет запускать контейнеры с инструментами lint и prettier для проверки кода в рамках CI/CD pipeline в Kubernetes кластере.

## 20. Может ли на сервере выполняться несколько джобов сразу? (k8s)

1) Да, на сервере может выполняться несколько джобов одновременно при одном поднятом GitLab Runner, если это настроено в конфигурации GitLab Runner. GitLab Runner может выполнять параллельно несколько джобов в зависимости от настроек concurrency в конфигурации GitLab Runner.

2) Утилизация контейнеров на сервере, где поднят GitLab Runner, зависит от настроек самого GitLab Runner и конфигурации джобов в `.gitlab-ci.yml`. Когда GitLab Runner выполняет джобы, он создает контейнеры для выполнения задач из определенных образов. После завершения выполнения джобы контейнеры могут быть остановлены или удалены в зависимости от настроек.

3) Отличия от запуска джобов на GitLab Runner, поднятом на сервере, и на GitLab Runner, поднятом в Kubernetes кластере, включают следующее:
   - **Масштабируемость:** В Kubernetes можно легко масштабировать количество GitLab Runner экземпляров в зависимости от нагрузки, что обеспечивает более высокую параллельность выполнения джобов.
   - **Изоляция:** В Kubernetes контейнеры с джобами могут быть запущены в изолированных подах, обеспечивая лучшую изоляцию и безопасность выполнения задач.
   - **Управление ресурсами:** В Kubernetes можно лучше управлять ресурсами, выделенными для выполнения джобов, такими как CPU и память, благодаря возможностям Kubernetes по управлению ресурсами контейнеров.
   - **Управление жизненным циклом:** Kubernetes обеспечивает удобное управление жизненным циклом контейнеров, их масштабированием и обновлением, что может быть удобнее для развертывания и управления GitLab Runner.

## 21. Перечислить список типов объектов в Kubernetes.

    1. `Pod`:
    Представляет собой наименьшую единицу развертывания в Kubernetes, содержащую один или несколько контейнеров.

    2. `Service`:
    Определяет набор подов и политику доступа к ним, создавая стабильные конечные точки для взаимодействия.

    3. `Volume`:
    Предоставляет постоянное хранилище для данных, доступное для подов в Kubernetes.

    4. `Namespace`:
    Используется для группировки ресурсов и управления доступом к ним внутри кластера.

    5. `ConfigMap`:
    Хранит конфигурационные данные, такие как параметры и настройки, которые могут быть использованы в приложениях.

    6. `Secret`:
    Содержит конфиденциальную информацию, такую как пароли, ключи API и сертификаты, обеспечивая их безопасное хранение и использование.

    7. `PersistentVolume`:
    Предоставляет управляемое постоянное хранилище, которое может быть использовано независимо от жизненного цикла подов.

    8. `PersistentVolumeClaim`:
    Запрос на использование постоянного хранилища, который позволяет подам получать доступ к PersistentVolume.

    9. `StorageClass`:
    Определяет класс хранилища, предоставляемый для PersistentVolume, включая параметры такие как тип хранилища и провайдер.

    10. `StatefulSet`:
    Управляет развертыванием и масштабированием приложений с постоянными и уникальными идентификаторами.

    11. `DaemonSet`:
    Обеспечивает запуск экземпляра пода на каждом узле в кластере.

    12. `ReplicaSet`:
    Обеспечивает желаемое количество реплик подов, обеспечивая их непрерывную работу.

    13. `Deployment`:
    Определяет желаемое состояние подов и контролирует их жизненный цикл, позволяя обновлять и масштабировать приложения.

    14. `Job`:
    Управляет выполнением задач, которые должны быть выполнены только один раз.

    15. `CronJob`:
    Позволяет запускать задачи периодически в установленное время.

    16. `Ingress`:
    Управляет внешним доступом к службам внутри кластера Kubernetes, позволяя настраивать маршрутизацию HTTP и HTTPS-трафика.

    17. `NetworkPolicy`:
    Определяет правила безопасности для сетевого трафика между подами и службами в кластере.

    18. `ServiceAccount`:
    Предоставляет идентификацию и авторизацию для подов в Kubernetes.

## 22. Назвать разницу между Deployment и StatefulSet.

    `Deployment` предназначен для управления развертыванием и масштабированием приложений с учетом непрерывной поставки и обновления.
    Поды, создаваемые с помощью Deployment, имеют уникальные идентификаторы, но не сохраняют постоянное состояние.
    При обновлении Deployment, новые версии приложений могут быть развернуты параллельно с текущими версиями, и после успешного развертывания новой версии, старые версии могут быть постепенно уничтожены.

    `StatefulSet` предназначен для управления развертыванием приложений, которые требуют уникальных и постоянных идентификаторов, таких как базы данных или другие приложения, хранящие постоянные данные.
    Поды, создаваемые с помощью StatefulSet, имеют уникальные и постоянные идентификаторы, что позволяет им сохранять свое состояние и поддерживать уникальные идентификаторы даже в случае перезапуска или масштабирования.
    StatefulSet также обеспечивает управление порядком запуска и остановки подов, что важно для приложений, зависящих от порядка инициализации или остановки.
    Таким образом, основное различие между Deployment и StatefulSet заключается в их способности обращаться с подами и обеспечивать сохранение состояния. Deployment предназначен для управления масштабируемыми приложениями без сохранения состояния, в то время как StatefulSet предназначен для управления приложениями, которые требуют уникальных и постоянных идентификаторов и поддержания состояния.

## 23. Рассказать что такое POD в k8s.

    Pod в Kubernetes - это наименьшая и базовая единица, которая представляет собой группу одного или нескольких контейнеров, которые работают вместе на одном узле. Контейнеры внутри Pod'a разделяют одинаковый набор ресурсов, таких как сеть и хранилище, и могут обмениваться информацией через общие тома.

    Основная разница между Pod'ом, виртуальной машиной и контейнером заключается в их уровне абстракции и изоляции:

    Контейнеры - это приложения, которые работают в изолированной среде, используя общие ресурсы операционной системы. Они легковесны и быстро запускаются, но не обеспечивают полной изоляции от других контейнеров на узле.

    Виртуальные машины - это полная виртуализация, где каждая ВМ имеет свою собственную операционную систему и оборудование, и работает в изолированной среде. ВМ более изолированы от других ВМ на узле, но требуют больше ресурсов и медленнее запускаются.

    Pod в Kubernetes - это группа контейнеров, которые работают в изолированной среде, используя общие ресурсы узла. Pod обеспечивает более высокий уровень изоляции, чем контейнеры, но более низкий, чем виртуальные машины.

    Таким образом, Pod в Kubernetes - это уровень абстракции между контейнером и виртуальной машиной, который обеспечивает изоляцию и общие ресурсы для группы контейнеров, работающих вместе на одном узле.

## 24. Зачем нужен k8s? (k8s)

`Kubernetes (k8s)` - это платформа для автоматизации развертывания, масштабирования и управления контейнеризированными приложениями. Основные причины использования Kubernetes:

1. `Оркестрация контейнеров`: Kubernetes упрощает управление большим количеством контейнеров, автоматически распределяя их по узлам, обеспечивая высокую доступность и отказоустойчивость.

2. `Масштабирование`: Kubernetes позволяет легко масштабировать приложения, как вертикально (увеличение ресурсов для одного экземпляра), так и горизонтально (увеличение числа экземпляров).

3. `Самовосстановление`: Kubernetes автоматически восстанавливает состояние системы в случае сбоев или ошибок.

4. `Декларативное управление`: Kubernetes позволяет описывать желаемое состояние системы в виде конфигурационных файлов, после чего самостоятельно поддерживает это состояние.

5. `Расширяемость`: Kubernetes имеет модульную архитектуру, что позволяет расширять его функционал с помощью плагинов и дополнительных ресурсов.

Kubernetes является базовой технологией для оркестрации контейнеров и обеспечивает основу для создания и управления современными микросервисными архитектурами.


## 25. Чем лучше оркестрация k8s в сравнении с systemd units и ansible, заменит ли оркестрацию? (k8s)

Kubernetes (k8s), systemd units и Ansible - это инструменты с разными целями и областями применения, и каждый из них имеет свои преимущества и недостатки.

`Kubernetes` предназначен для оркестрации контейнеризированных приложений, обеспечивая автоматизацию развертывания, масштабирования и управления контейнерами. Он обладает богатыми возможностями для управления микросервисами, автоматического восстановления после сбоев, масштабирования и декларативного управления.

`systemd units` - это инструмент для управления службами и процессами в операционной системе Linux. Он позволяет запускать, останавливать и управлять отдельными службами, но не обладает возможностями оркестрации и управления распределенными приложениями.

`Ansible` - это инструмент для автоматизации конфигурации и управления серверами. Он позволяет описывать инфраструктуру в виде кода и автоматизировать процессы развертывания и управления серверами, но не обладает возможностями оркестрации контейнеров.

Kubernetes не заменяет systemd units и Ansible, а дополняет их в области управления контейнеризированными приложениями. В то же время, для управления инфраструктурой и серверами, Ansible остается полезным инструментом, а systemd units используется для управления службами внутри операционной системы.

`Можно использовать systemd units для запуска контейнеров, а затем использовать Ansible для их управления и оркестрации.` Это может быть полезным в определенных сценариях, особенно если у вас уже есть существующая инфраструктура, основанная на systemd units, и вы хотите постепенно внедрять контейнеризацию.

Однако Kubernetes все же предоставляет более широкие возможности для оркестрации контейнеров, такие как `автоматическое масштабирование`, `управление сетью`, `обновления без простоев` и многое другое. Кроме того, Kubernetes имеет богатый экосистемный набор инструментов и решений, который облегчает управление контейнеризированными приложениями в масштабе.

Таким образом, хотя использование systemd units и Ansible для оркестрации контейнеров возможно, Kubernetes все же предоставляет более полное и интегрированное решение для управления контейнеризированными приложениями.

## 26. Удавалось ли самостоятельно разворачивать k8s на bare metal? Если да - то через что? (k8s)

Возможно развернуть Kubernetes (k8s) на bare metal (физических серверах) без использования облачных провайдеров. Для этого можно использовать различные инструменты и подходы. Один из популярных инструментов для развертывания Kubernetes на bare metal - это **kubeadm**.

Шаги для развертывания Kubernetes на bare metal с использованием **kubeadm**:

1. **Подготовка серверов**:
   - Установите операционную систему на каждом сервере (например, Ubuntu, CentOS).
   - Убедитесь, что серверы могут связываться друг с другом по сети.

2. **Установка Docker и kubeadm**:
   - Установите Docker на каждом сервере.
   - Установите kubeadm, kubelet и kubectl на каждом сервере.

3. **Инициализация кластера**:
   - На одном из серверов выполните команду `kubeadm init` для инициализации кластера Kubernetes.
   - Следуйте инструкциям, чтобы добавить необходимые параметры.

4. **Присоединение рабочих узлов**:
   - На остальных серверах выполните команду `kubeadm join`, указав токен, который был сгенерирован при инициализации.

5. **Установка сетевого плагина**:
   - Установите сетевой плагин (например, Calico, Flannel) для обеспечения сетевой связности между подами.

Это базовый пример процесса развертывания Kubernetes на bare metal с использованием kubeadm. Для более сложных сценариев и настройки существуют другие инструменты, такие как kubespray, Rancher, и т. д. Каждый из них имеет свои особенности и требования.

https://github.com/JavaScriptonit/kubespray-ansible - Steps to deploy kubespray-ansible

## 27. Из каких служб состоит k8s? (k8s)

Kubernetes (k8s) состоит из следующих основных компонентов:

1. **kube-apiserver**: Компонент, который предоставляет API для управления кластером Kubernetes. Все операции в Kubernetes проходят через kube-apiserver.

2. **kube-controller-manager**: Компонент, который запускает различные контроллеры, отвечающие за управление состоянием кластера, контролируя ресурсы и обеспечивая желаемое состояние.

3. **kube-scheduler**: Компонент, который отвечает за планирование запуска подов на рабочих узлах в кластере.

4. **kubelet**: Агент, который работает на каждом рабочем узле и отвечает за управление контейнерами и подами на этом узле.

5. **kube-proxy**: Компонент, который обеспечивает сетевую проксировку в кластере и управляет сетевым доступом к сервисам.

6. **etcd**: Распределенное хранилище ключ-значение, которое используется Kubernetes для хранения всех данных конфигурации кластера.

Эти основные компоненты обеспечивают функциональность и управление кластером Kubernetes. Кроме того, существуют и другие дополнительные компоненты, такие как DNS-сервер, мониторинг и логирование, которые могут быть интегрированы в кластер для дополнительной функциональности и возможностей.

## 28. Может ли быть в pod больше 1го контейнера? И когда? (k8s)

Да, в Kubernetes в одном Pod может быть запущено более одного контейнера. Это называется **Multi-Container Pods**. Обычно в Pod находится один основной контейнер, который выполняет основную функцию приложения, и дополнительные вспомогательные контейнеры, которые могут выполнять различные задачи в поддержку основного контейнера.

Вот несколько причин, почему в Pod могут быть размещены несколько контейнеров:

1. **Совместное использование ресурсов**: Дополнительные контейнеры могут использовать общие ресурсы с основным контейнером, такие как файлы, сеть или тома данных.

2. **Совместная работа**: Контейнеры в одном Pod могут взаимодействовать друг с другом напрямую через локальный интерфейс, облегчая совместную работу и обмен данными.

3. **Шаблонные паттерны**: Например, контейнеры-сайдкары могут использоваться для добавления дополнительной функциональности, такой как логирование, мониторинг или обновление конфигурации, к основному контейнеру без изменения его кода.

4. **Разделение ответственности**: Разные контейнеры могут быть ответственны за различные аспекты работы приложения, что может способствовать улучшению модульности и облегчению управления.

Хотя Multi-Container Pods могут быть полезны в некоторых случаях, их следует использовать осторожно, чтобы не усложнять развертывание и управление. Каждый контейнер в Pod должен выполнять четко определенную задачу и быть легко масштабируемым и управляемым.

## 29. В каких ситуациях может DS пригодиться? Что такое DaemonSet? (k8s)

DaemonSet (DS) в Kubernetes - это ресурс, который гарантирует, что на каждом узле кластера будет запущен экземпляр пода. DS может быть полезен во многих ситуациях, включая:

1. **Сетевые сервисы**: DS может использоваться для развертывания сетевых сервисов, таких как сетевые прокси, мониторинговые агенты или сетевые решения безопасности, на каждом узле кластера.

2. **Логирование и мониторинг**: DS может быть использован для развертывания агентов логирования или мониторинга на каждом узле, чтобы обеспечить сбор и анализ данных с узлов кластера.

3. **Хранение данных**: DS может быть использован для развертывания узлов хранения данных на каждом узле, чтобы обеспечить доступ к данным на уровне узла.

4. **Системные агенты**: DS может быть использован для развертывания системных агентов, таких как агенты обновления, антивирусные программы или агенты управления узлами, на каждом узле кластера.

5. **Распределенные приложения**: DS может быть использован для развертывания частей распределенного приложения на каждом узле кластера, чтобы обеспечить доступность и отказоустойчивость.

DS обеспечивает удобный способ развертывания и управления приложениями, которые должны работать на каждом узле кластера. Он гарантирует, что приложение будет запущено на каждом узле и обеспечивает автоматическое масштабирование и обновление.

## 30. Создан pod. Статус - "Image pull back off" (образ не смог стянуть). Это приватный реестр. Куда смотреть? Как исправить? (k8s)

Когда Pod не может загрузить образ из приватного реестра, это может быть вызвано различными причинами. Вот несколько шагов, которые можно предпринять, чтобы исправить проблему:

1. **Проверьте доступ к реестру**: Убедитесь, что кластер Kubernetes имеет доступ к приватному реестру образов. Это может потребовать настройки правильных учетных данных (логин/пароль или токен) или использование секретов Kubernetes для доступа к реестру.

2. **Проверьте правильность имени образа**: Убедитесь, что имя образа в манифесте Pod правильно указано и соответствует образу в приватном реестре.

3. **Проверьте наличие секретов**: Если для доступа к приватному реестру требуются учетные данные, убедитесь, что в кластере созданы и правильно настроены секреты Kubernetes.

4. **Проверьте настройки безопасности**: Проверьте, нет ли ограничений на доступ к приватному реестру из кластера Kubernetes, таких как сетевые политики или правила файрвола.

5. **Проверьте логи контейнера**: Просмотрите логи контейнера, чтобы увидеть более подробную информацию о проблеме с загрузкой образа. Это может помочь выявить конкретную ошибку.

6. **Попробуйте повторно запустить Pod**: Иногда проблема с загрузкой образа может быть временной. Попробуйте повторно запустить Pod и посмотрите, устранится ли проблема.

7. **Используйте Docker-конфиг**: Если используется Docker-конфиг для доступа к приватному реестру, убедитесь, что он правильно сконфигурирован и доступен в кластере Kubernetes.

### Из личного опыта: 

1. Проблема с сертификатом. Нужно проверить его наличие, создать новый
2. Проблема с прокси. Посмотреть настройки прокси на сервере, в контейнере. Проверить сетевой доступ до реестра через прокси или убрать прокси из конфигов.

## 31. Создали ingress, который смотрит на сервис, а сервис смотрит на поды. Делая запрос на ingress - получаем ошибку. Запрос до pod не доходит (запроса нет в логах пода). Как исправить? Куда смотреть? (k8s)

Когда запросы не доходят до Pod через Ingress в Kubernetes, это может быть вызвано различными причинами. Вот несколько шагов, которые можно предпринять для анализа и исправления проблемы:

1. **Проверьте конфигурацию Ingress**:
   - Убедитесь, что правильно настроены правила маршрутизации в Ingress для отправки запросов на нужный сервис.
   - Проверьте, что хост и путь в Ingress соответствуют вашему запросу.

2. **Проверьте конфигурацию сервиса**:
   - Убедитесь, что сервис правильно настроен для перенаправления трафика на поды.
   - Проверьте, что селекторы сервиса соответствуют меткам Pod.

3. **Проверьте конфигурацию Pod**:
   - Убедитесь, что Pod правильно настроен и работает корректно.
   - Проверьте логи Pod, чтобы убедиться, что запросы доходят до Pod и обрабатываются правильно.

4. **Проверьте сетевую конфигурацию**:
   - Убедитесь, что сетевые политики Kubernetes не блокируют трафик между Ingress, сервисами и Pod.
   - Проверьте доступность сетевых портов и правила файрвола.

5. **Используйте утилиты для диагностики**:
   - Используйте `kubectl describe` для проверки статуса Ingress, сервисов и Pod.
   - Используйте `kubectl logs` для просмотра логов Ingress, сервисов и Pod.

6. **Проверьте статусы ресурсов**:
   - Проверьте статус Ingress, сервисов и Pod с помощью `kubectl get`.
   - Убедитесь, что все ресурсы находятся в состоянии "Running" или "Active".

7. **Попробуйте перезапустить ресурсы**:
   - Попробуйте перезапустить Ingress, сервисы и Pod, чтобы устранить возможные проблемы с текущими экземплярами.

## 32. Какие отличия DaemonSet от Deployment и StatefulSet? (k8s)

DaemonSet, Деплоймент (Deployment) и StatefulSet - это три разных ресурса в Kubernetes, каждый из которых предназначен для управления приложениями и контейнерами в кластере Kubernetes, но у них есть различия в том, как они работают и для каких целей они предназначены.

1. **DaemonSet**:
   - DaemonSet обеспечивает запуск экземпляра приложения на каждом узле в кластере.
   - Это полезно для задач, которые должны быть запущены на каждом узле, например, мониторинг, логирование или хранение данных.
   - Если узел добавляется в кластер, DaemonSet автоматически запускает экземплят приложения на новом узле.
   
2. **Деплоймент (Deployment)**:
   - Деплоймент обеспечивает управление ресурсами в виде реплик набора подов (Pod), позволяя управлять переключением на новые версии приложений или масштабированием количества реплик.
   - Это основной способ управления развертыванием приложений в Kubernetes с возможностью изменения состояния.
   
3. **StatefulSet**:
   - StatefulSet похож на Деплоймент, но предназначен для stateful приложений, которые требуют уникальной идентификации и устойчивого хранения данных, таких как базы данных.
   - StatefulSet обеспечивает гарантии порядка, уникальные имена и устойчивые идентификаторы для подов.

Каждый из них предназначен для решения определенных задач и сценариев использования в Kubernetes. Каждый из этих ресурсов обладает уникальными особенностями и функциональностью, которая делает их подходящими для различных типов приложений и требований. Например, DaemonSet полезен для разворачивания служб на каждом узле, Деплоймент хорошо подходит для обновления приложений и контроля над версиями, а StatefulSet подходит для работы с состоянием приложений с уникальными идентификаторами.

## 33. Как происходит создание ролей в кластере? Service Account, Role Binding, Role. Чем они отличаются? (k8s)

Создание ролей в кластере Kubernetes включает в себя создание нескольких объектов для управления доступом к ресурсам в кластере. Зачастую используется следующий набор объектов:

1. **Сервис-аккаунт (Service Account)**:
   - Сервис-аккаунт представляет собой идентификатор для приложений и сервисов, работающих внутри кластера Kubernetes.
   - Он используется для аутентификации и авторизации в Kubernetes API.

2. **Роль (Role)** и **Роль Биндинг (Role Binding)**:
   - Роль определяет права доступа к определенным ресурсам в Kubernetes (например, поды, службы) в рамках одного namespace.
   - Роль Биндинг связывает определенного пользователя, группу или сервис-аккаунт с конкретной ролью в одном namespace.

3. **Кластерная Роль (Cluster Role)** и **Кластерный Роль Биндинг (Cluster Role Binding)**:
   - Кластерная Роль аналогична Роли, но действует на уровне всего кластера, а не только на уровне одного namespace.
   - Кластерный Роль Биндинг связывает пользователя, группу или сервис-аккаунт с кластерной ролью и действует на уровне всего кластера.

Отличие между Ролью и Кластерной Ролью заключается в области применения прав доступа: Роль применяется к ресурсам в одном namespace, в то время как Кластерная Роль применяется к ресурсам на уровне всего кластера. Точно также Роль Биндинг и Кластерный Роль Биндинг связывают пользователей, группы или сервис-аккаунты с соответствующими ролями на определенном уровне (namespace или весь кластер).

Использование разных уровней доступа (namespace и весь кластер) позволяет гибко управлять правами доступа в Kubernetes, обеспечивая необходимую гранулярность и безопасность в рамках кластера. К примеру, Роль может быть полезна для установки прав доступа конкретному приложению в рамках одного namespace, в то время как Кластерная Роль может задавать права для администрирования всего кластера.

## 34. Какие отличия между docker/dockerd/containerd? (k8s)

Docker, Dockerd и Containerd - это все элементы, связанные с контейнеризацией и управлением контейнерами, но у них есть некоторые ключевые различия:

1. **Docker**:
   - Docker является популярным инструментом для контейнеризации приложений. Он предоставляет полный стек инструментов для работы с контейнерами, включая средства управления образами, контейнерами, сетями и томами данных.
   - Docker также включает **Docker Engine (dockerd)**, который является серверной частью Docker, отвечающей за управление контейнерами.

2. **Containerd**:
   - Containerd является легковесным компонентом, который является основой для запуска контейнеров. Он предоставляет базовый функционал для управления жизненным циклом контейнеров, таких как запуск, остановка, создание и удаление.
   - Containerd нацелен на работу как в составе Docker, так и в других контейнерных решениях.

Касательно Kubernetes и Docker:
- Кubernetes использует Containerd в качестве контейнерного рантайма для работы с контейнерами внутри подов. Kubernetes прямо взаимодействует с Containerd, чтобы управлять жизненным циклом контейнеров.
- Kubernetes прекратил использование Docker как рантайма для контейнеров с версии 1.20, из-за сложности интеграции между Docker и Kubernetes, и затем его поддержка была полностью удалена в версии 1.22. Вместо Docker, Kubernetes рекомендует использовать Containerd или другие совместимые контейнерные рантаймы для работы с контейнерами.

Таким образом, Kubernetes может использовать Containerd для управления контейнерами в подах, именно поэтому Docker как рантайм больше не требуется для работы с Kubernetes.

## 35. 